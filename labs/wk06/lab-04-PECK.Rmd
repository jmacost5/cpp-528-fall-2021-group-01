---
title: "Lab 04"
author: "April Peck"
date: "11/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load necessary packages ----
library( dplyr )
library( here )
library( knitr )
library( pander )
library( stargazer )
library( scales )


# load necessary functions and objects ----
# note: all of these are R objects that will be used throughout this .rmd file
import::here("panel.cor", 
             "panel.smooth",
             "jplot",
             "d",
             "df",
             "cbsa_stats_df",
             "INFLATION_RATE",
             "s.type",
             # notice the use of here::here() that points to the .R file
             # where all these R objects are created
             .from = here::here("labs/wk06/labs_04_05_source-amp.R"),
             .character_only = TRUE)

```


```{r, echo=FALSE}

# Part 1 - Data
#Similar to your previous lab, create a dataset that includes 2000 and 2010 census variables drop all rural census tracts.

#d1 <- readRDS( here::here( "data/rodeo/LTDB-2000.rds" ) )
#d2 <- readRDS( here::here( "data/rodeo/LTDB-2010.rds" ) )
#md <- readRDS( here::here( "data/rodeo/LTDB-META-DATA.rds" ) )

#d1 <- dplyr::select( d1, - year )
#d2 <- dplyr::select( d2, - year )

#d <- merge( d1, d2, by="tractid" )
#d <- merge( d, md, by="tractid" )

# filter rural districts
#d <- dplyr::filter( d, urban == "urban" )

```


```{r, echo=FALSE}
#d <- dplyr::select( d, tractid, 
#                    mhmval00, mhmval12, 
#                    hinc00, clf00, npov00, dpov00,
#                    vac00, own00, rent00, h30old00,
#                    empclf00, unemp00, prof00,  
#                   ag25up00, hs00, col00, hu00,
#                    pop00.x, 
#                    cbsa, cbsaname )


#d <- 
#  d %>%
#  dplyr::mutate( p.hs = 100 * (hs00+col00) / ag25up00,
#                 p.unemp = 100 * unemp00 / clf00,
#                 p.vacant = 100 * vac00 / hu00,
#                 mhv.change.00.to.10 = mhmval12 - mhmval00,
#                 p.mhv.change = 100 * (mhmval12 - mhmval00) / mhmval00,
#                 pov.rate = 100 * npov00 / dpov00 )

# adjust 2000 home values for inflation 
#mhv.00 <- d$mhmval00 * INFLATION_RATE 
#mhv.10 <- d$mhmval12
```



```{r, echo=FALSE}
#Create a variable that measures the growth of median home value from 2000 to 2010.


# change in MHV in dollars
#mhv.change <- mhv.10 - mhv.00

# change in MHV in percent
#mhv.growth <- 100 * ( mhv.change / mhv.00 )
```


```{r, echo=FALSE}

#Omit cases that have a median home value less than $10,000 in 2000.

#mhv.00[ mhv.00 < 10000 ] <- NA

```

```{r, echo=FALSE}
# Omit cases with growth rates above 200%.
#mhv.growth[ mhv.growth > 2 ] <- NA

#d$mhv.00 <- mhv.00
#d$mhv.10 <- mhv.10
#d$mhv.change <- mhv.change
#d$mhv.growth <- mhv.growth 

# create mini data frame
#df <- data.frame( MedianHomeValue2000=mhv.00, 
#                  MedianHomeValue2010=mhv.10, 
#                  MHV.Change.00.to.10=mhv.change,
#                  MHV.Growth.00.to.12=mhv.growth )

# average growth in median home value for the city
#cbsa_stats_df <- 
#  d %>%
#  dplyr::group_by( cbsaname ) %>%
#  dplyr::summarize( metro.mhv.change = median( mhv.change, na.rm=T ),
#                    metro.mhv.growth = 100 * median( mhv.growth, na.rm=T ) ) %>%
#  dplyr::ungroup() 

```

# Part 2 - Predict MHV Change
Select at least three census variables that you feel will be good predictors of change in MHV between 2000 and 2010.

1. p.vacant
2. p.hs
3. p.unemp


Run the model while including metro-level fixed effects (cbsa name or FIPS). Make sure you check for variable skew and multicollinearity and adjust accordingly.



#### Check for variable skew

```{r}
# p.vacant

par(mfrow = c(1,2))
hist( d$p.vacant.00, breaks=50, col="gray20", border="white",
      yaxt="n", xlab="", ylab="", main="% Vacant Homes")

hist( log(d$p.vacant.00 + 1 ), breaks=50, col="gray20", border="white",
      yaxt="n", xlab="", ylab="", main="% Vacant Homes (Logged) ")
```

```{r}
log.p.vacant <- log10( d$p.vacant.00 + 1)

par(mfrow = c(1,2))

hist( d$p.vacant.00, breaks=50, col="gray20", border="white",
      yaxt="n", xlab="", ylab="", main="% Vacant Homes")

hist( log.p.vacant, breaks=50, col="gray20", border="white",
      yaxt="n", xlab="", ylab="", main="% Vacant Homes (Logged) ")
```

```{r}
#p.hs.00

par(mfrow = c(1,2))

hist( d$p.hs.00, breaks=50, col="gray20", border="white",
      yaxt="n", xlab="", ylab="", main="Percent High School Degree+")

hist( log(d$p.hs.00+1), breaks=50, col="gray20", border="white",
      yaxt="n", xlab="", ylab="", main="Percent High School Degree+ (Logged)")
```


```{r}

#p.unemp

par(mfrow = c(1,2))

hist( d$p.unemp.00, breaks=50, col="gray20", border="white",
      yaxt="n", xlab="", ylab="", main="Percent Unemployed")

hist( log(d$p.unemp.00+1), breaks=50, col="gray20", border="white",
      yaxt="n", xlab="", ylab="", main="Percent Unemployed (Logged)")
```

```{r}
log.p.unemp <- log10( d$p.unemp.00 + 1)

par(mfrow = c(1,2))

hist( d$p.unemp.00, breaks=50, col="gray20", border="white",
      yaxt="n", xlab="", ylab="", main="Percent Unemployed")

hist( log.p.unemp, breaks=50, col="gray20", border="white",
      yaxt="n", xlab="", ylab="", main="Percent Unemployed (Logged)")
```

```{r}
# create subset to visualize in correlation matrix
d2 <- select( d, mhv.growth, p.vacant.00, p.hs.00, p.unemp.00)

# reduce data density for visualization
set.seed( 1234 )
d3 <- sample_n( d2, 10000 ) %>% na.omit()
# correlation plots
pairs( d3, upper.panel=panel.cor, lower.panel=panel.smooth )
```

```{r}
# recode some vars to remove outliers and skew
d2$mhv.growth[ d2$mhv.growth > 200 ] <- NA
d2$p.vacant.00 <- log10( d2$p.vacant.00 + 1)
d2$p.unemp.00 <- log10( d2$p.unemp.00 + 1 )


# reduce data density for visualization
set.seed( 1234 )
d4 <- sample_n( d2, 10000 ) %>% na.omit()
# correlation plots
pairs( d4, upper.panel=panel.cor, lower.panel=panel.smooth )
```

#### Check for multicolinearity

A sign of multicollinarity is when both coefficients get smaller and the standard errors get bigger when the two variables are included in the same model 
```{r, results = 'asis'}

#set up data
reg.data <- d

reg.data$mhv.growth[reg.data$mhv.growth > 200] <- NA
reg.data$p.vacant.00 <- log10(reg.data$p.vacant.00 + 1)
reg.data$p.unemp.00 <- log10(reg.data$p.unemp.00 + 1)
reg.data$p.hs.00 <- log10(reg.data$p.hs.00 + 1)

#check for multicolinearity: p.vacant / p.hs

m1 <- lm( mhv.growth ~ p.vacant.00 + cbsa, data = reg.data )
m2 <- lm( mhv.growth ~ p.hs.00 + cbsa, data = reg.data )
m4 <- lm( mhv.growth ~ p.vacant.00 + p.hs.00 + cbsa, data = reg.data )

stargazer( m1, m2, m4,
           type = s.type, digits = 2,
           omit.stat = c("rsq","f"),
           omit = "cbsa")

```

Both coefficients increased and there was no change to the standard errors to speak of. I don't believe there's an issue with multicollinearity.

```{r, results = 'asis'}

#check for multicolinearity: p.vacant / p.unemp
m3 <- lm( mhv.growth ~ p.unemp.00 + cbsa, data = reg.data )
m5 <- lm( mhv.growth ~ p.vacant.00 + p.unemp.00 + cbsa, data = reg.data )

stargazer( m1, m3, m5,
           type = s.type, digits = 2,
           omit.stat = c("rsq","f"),
           omit = "cbsa")

```


Both coefficients got larger, and there was no real change to the standard errors. No issue with multicollinearity.

```{r, results = 'asis'}
#check for multicolinearity: p.unemp / p.hs
m6 <- lm( mhv.growth ~ p.unemp.00 + p.hs.00 + cbsa, data = reg.data )

stargazer( m3, m2, m6,
           type = s.type, digits = 2,
           omit.stat = c("rsq","f"),
           omit = "cbsa")

```

Both coefficients increased (minimally for p.hs); and the standard error for p.unemp did not change much while p.hs increased a bit more. I don't believe there's an issue with multicolinearity

```{r, results = 'asis'}

m7 <- lm( mhv.growth ~ p.unemp.00 + p.hs.00 + p.vacant.00 + cbsa, data = reg.data )

stargazer( m1, m2, m3, m7,
           type = s.type, digits = 2,
           omit.stat = c("rsq","f"),
           omit = "cbsa")

```


#### Discussion

What are the results? Which factor was most important? Did it meet your expectations? Were there any variables that were not significant that you expected to be?
*I did not find multicolinearity with any of my coefficients. The percent unemployed was the most important factor in determining the change in home values between 2000 and 2010. I was surprised to find the high school degree was not statistically significant before including the metro-level data but became significant when I added it*

Explain your findings to a general audience.
*Vacancy, education, and unemployment all affect the growth in home values. The three variables are each negatively correlated with increase in values, meaning as each variable increases, growth in home values decreases; each variable decreases home values. Metro-level data was taken into account when looking at the relationship, so each variable was looked to see how it affected the home value growth within its metro area, and each area aggregated. The relationship is statistically significant with a p-level of 0.05.*